---
title: 'Configuration Overview'
description: 'Complete guide to configuring ylog for your workflow'
---

## Configuration File

ylog uses a JavaScript configuration file that provides flexibility and type safety:

```javascript
// ylog.config.js
export default {
  github: {
    repo: 'owner/repository',      // Auto-detected from git remote
    token: 'ghp_your_token',       // Optional, uses auth hierarchy
    throttleRpm: 100               // GitHub API rate limit
  },
  ai: {
    provider: 'ollama',            // 'ollama' | 'anthropic'
    model: 'llama3.2',            // Model name
    endpoint: 'http://localhost:11434',  // Ollama endpoint
    apiKey: 'sk-...',             // Anthropic API key
    maxTokens: 100                // Max tokens for AI responses
  },
  concurrency: 10,                // Parallel processing limit
  outputDir: '.ylog',            // Database storage directory
  generateContextFiles: true,     // Enable .ylog file generation
  contextFileThreshold: 3,        // Min PRs for context file
  historyMonths: 6,              // Recent changes window
  cacheDir: '.ylog/cache',       // API response cache
  diffMaxBytes: 1048576          // Max diff size (1MB)
};
```

## Configuration Sources

ylog looks for configuration in this order:

<Steps>
  <Step title="Command Line Flag">
    ```bash
    ylog sync --config ./custom-config.js
    ```
    Highest priority - explicit override
  </Step>
  <Step title="ylog.config.js">
    ```bash
    ./ylog.config.js
    ```
    Default location in current directory
  </Step>
  <Step title="ylog.config.json">
    ```bash
    ./ylog.config.json
    ```
    JSON alternative to JavaScript config
  </Step>
  <Step title="Default Values">
    Built-in defaults for all options
  </Step>
</Steps>

## Core Configuration Sections

<AccordionGroup>
  <Accordion title="GitHub Configuration">
    ```javascript
    github: {
      repo: 'owner/repository',    // Repository to process
      token: 'ghp_your_token',     // GitHub Personal Access Token
      throttleRpm: 100             // API requests per minute
    }
    ```
    
    **Auto-detection:**
    - `repo` is auto-detected from `git remote get-url origin`
    - `token` uses authentication hierarchy (env var → config → gh CLI)
    
    **Rate limiting:**
    - GitHub allows 5000 requests/hour for authenticated users
    - `throttleRpm` prevents hitting secondary rate limits
    - Lower values for shared tokens or large repositories
  </Accordion>

  <Accordion title="AI Configuration">
    ```javascript
    ai: {
      provider: 'ollama',          // AI provider choice
      model: 'llama3.2',          // Specific model
      endpoint: 'http://localhost:11434',  // Ollama only
      apiKey: 'sk-ant-...',       // Anthropic only
      maxTokens: 100              // Response length limit
    }
    ```
    
    **Providers supported:**
    - **Ollama**: Local AI, free, requires installation
    - **Anthropic**: Cloud AI, paid, higher quality
    
    **Model selection:**
    - Larger models = better summaries but slower processing
    - Smaller models = faster processing but less detailed
  </Accordion>

  <Accordion title="Output Configuration">
    ```javascript
    outputDir: '.ylog',             // Database directory
    generateContextFiles: true,     // Enable .ylog files
    contextFileThreshold: 3,        // Min PRs for context file
    historyMonths: 6               // Recent changes window
    ```
    
    **Directory structure:**
    ```
    .ylog/
    ├── prs.db              # SQLite database
    └── cache/              # API response cache
    ```
    
    **Context file tuning:**
    - Higher threshold = fewer, more focused files
    - Longer history = more context but larger files
  </Accordion>
</AccordionGroup>

## Environment-Specific Configs

<Tabs>
  <Tab title="Development">
    ```javascript
    // ylog.config.dev.js
    export default {
      ai: {
        provider: 'ollama',
        model: 'llama3.2',
        endpoint: 'http://localhost:11434'
      },
      concurrency: 5,              // Lower for local development
      generateContextFiles: true,
      contextFileThreshold: 2      // More granular for exploration
    };
    ```
    
    **Usage:**
    ```bash
    ylog sync --config ylog.config.dev.js
    ```
  </Tab>
  
  <Tab title="Production/CI">
    ```javascript
    // ylog.config.prod.js
    export default {
      ai: {
        provider: 'anthropic',
        model: 'claude-3-haiku-20240307',
        apiKey: process.env.ANTHROPIC_API_KEY
      },
      concurrency: 20,             // Higher for CI environments
      generateContextFiles: true,
      contextFileThreshold: 5,     // Focus on major areas
      historyMonths: 12           // Longer historical view
    };
    ```
  </Tab>
  
  <Tab title="Large Repository">
    ```javascript
    // ylog.config.large.js
    export default {
      github: {
        throttleRpm: 50            // Conservative rate limiting
      },
      ai: {
        provider: 'anthropic',     // Faster processing
        model: 'claude-3-haiku-20240307'
      },
      concurrency: 30,            // Higher parallelism
      diffMaxBytes: 512000,       // Smaller diff limit (512KB)
      contextFileThreshold: 10    // Only major areas
    };
    ```
  </Tab>
</Tabs>

## Configuration Validation

ylog validates your configuration at startup:

```bash
ylog sync

# Validation output:
✅ Configuration loaded: ./ylog.config.js
✅ GitHub repository: owner/repo (auto-detected)
✅ AI provider: ollama (llama3.2)
✅ Output directory: .ylog
```

**Common validation errors:**

<AccordionGroup>
  <Accordion title="Invalid Repository Format">
    ```bash
    ❌ Invalid repository format: "my-repo"
    Expected format: "owner/repository"
    ```
    
    **Fix:** Use full owner/repository format or let ylog auto-detect
  </Accordion>

  <Accordion title="Missing AI Configuration">
    ```bash
    ❌ AI configuration required
    Please specify provider and model
    ```
    
    **Fix:** Add complete `ai` section to config
  </Accordion>

  <Accordion title="Invalid Model">
    ```bash
    ❌ Model "invalid-model" not available
    Available models: llama3.2, llama3.1, codellama
    ```
    
    **Fix:** Check available models with `ollama list` or Anthropic docs
  </Accordion>
</AccordionGroup>

## Advanced Configuration

### Custom Prompts

```javascript
// Advanced AI configuration
ai: {
  provider: 'ollama',
  model: 'llama3.2',
  customPrompts: {
    prSummary: `
      Analyze this pull request and provide:
      1. Business motivation (why was this needed?)
      2. Technical approach (how was it implemented?)
      3. Impact areas (what parts of the system are affected?)
      
      Be concise but informative.
    `
  }
}
```

### Performance Tuning

```javascript
// Performance optimization
export default {
  // GitHub API optimization
  github: {
    throttleRpm: 150,           // Increase if you have dedicated tokens
    batchSize: 25               // PRs per batch
  },
  
  // AI processing optimization
  ai: {
    maxTokens: 150,             // Longer responses
    temperature: 0.3,           // More focused responses
    concurrentRequests: 5       // Parallel AI requests
  },
  
  // System optimization
  concurrency: 15,              // Total parallelism
  cacheDir: '/tmp/ylog-cache', // Faster filesystem
  diffMaxBytes: 2097152        // 2MB diff limit
};
```

### Repository-Specific Settings

```javascript
// Monorepo configuration
export default {
  // Override area detection for monorepos
  areaMapping: {
    'packages/frontend/': 'frontend',
    'packages/backend/': 'backend',
    'packages/shared/': 'shared',
    'apps/mobile/': 'mobile'
  },
  
  // Exclude certain paths
  excludePaths: [
    'node_modules/**',
    'dist/**',
    '**/*.test.js',
    'docs/**'
  ],
  
  // Custom context file locations
  contextFiles: {
    'packages/frontend/': 'packages/frontend/CONTEXT.md',
    'packages/backend/': 'packages/backend/CONTEXT.md'
  }
};
```

## Configuration Templates

<CodeGroup>

```javascript Minimal Setup
// Quick start configuration
export default {
  ai: {
    provider: 'ollama',
    model: 'llama3.2'
  }
};
```

```javascript Team Collaboration
// Team-friendly configuration
export default {
  github: {
    throttleRpm: 80              // Conservative for shared tokens
  },
  ai: {
    provider: 'anthropic',       // Consistent results
    model: 'claude-3-haiku-20240307'
  },
  generateContextFiles: true,
  contextFileThreshold: 3,
  historyMonths: 6
};
```

```javascript Enterprise Setup
// Enterprise-grade configuration
export default {
  github: {
    throttleRpm: 200,           // Dedicated API limits
    batchSize: 50
  },
  ai: {
    provider: 'anthropic',
    model: 'claude-3-sonnet-20240229',  // Higher quality
    maxTokens: 200
  },
  concurrency: 25,
  generateContextFiles: true,
  contextFileThreshold: 5,
  historyMonths: 12,
  outputDir: '/shared/ylog-data'
};
```

</CodeGroup>

## Best Practices

<AccordionGroup>
  <Accordion title="Security">
    **DO:**
    - Use environment variables for tokens and API keys
    - Keep config files in version control (without secrets)
    - Use least-privilege tokens
    
    **DON'T:**
    - Commit API keys or tokens
    - Use overly broad GitHub token scopes
    - Share configuration with embedded secrets
  </Accordion>

  <Accordion title="Performance">
    **For large repositories:**
    - Lower `throttleRpm` to avoid rate limits
    - Increase `concurrency` for faster processing
    - Use cloud AI providers for speed
    - Consider `diffMaxBytes` limits
    
    **For small teams:**
    - Higher `throttleRpm` for faster syncs
    - Local AI providers to save costs
    - More granular context files
  </Accordion>

  <Accordion title="Maintenance">
    **Regular updates:**
    - Review and update AI models periodically
    - Adjust thresholds based on repository growth
    - Monitor rate limit usage
    - Clean cache directories occasionally
    
    **Team coordination:**
    - Document configuration choices
    - Share optimal settings across team
    - Version control configuration files
  </Accordion>
</AccordionGroup>

## Troubleshooting Configuration

<AccordionGroup>
  <Accordion title="Configuration Not Found">
    ```bash
    ⚠️ No configuration file found, using defaults
    ```
    
    **Solutions:**
    1. Run `ylog init` to create default config
    2. Create `ylog.config.js` manually
    3. Use `--config` flag to specify location
  </Accordion>

  <Accordion title="Invalid Configuration">
    ```bash
    ❌ Configuration validation failed: Invalid AI provider "openai"
    ```
    
    **Solutions:**
    1. Check configuration syntax
    2. Verify enum values (provider, model names)
    3. Ensure required fields are present
    4. Use `ylog init --force` to regenerate
  </Accordion>

  <Accordion title="Permission Issues">
    ```bash
    ❌ Cannot write to output directory: .ylog
    ```
    
    **Solutions:**
    1. Check directory permissions
    2. Choose different `outputDir`
    3. Run with appropriate permissions
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="GitHub Configuration" href="/configuration/github">
    Detailed GitHub-specific settings
  </Card>
  <Card title="AI Providers" href="/configuration/ai-providers">
    Complete AI provider configuration guide
  </Card>
  <Card title="Advanced Settings" href="/configuration/advanced">
    Performance tuning and customization
  </Card>
  <Card title="Quick Start" href="/quickstart">
    Get up and running quickly
  </Card>
</CardGroup>